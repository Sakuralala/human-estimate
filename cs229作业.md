*作业1*  
1.In this problem, we will prove that if we use Newton’s method solve the least squares
optimization problem, then we only need one iteration to converge to $\theta^*$.  
(a) Find the Hessian of the cost function $J(θ)=\frac{1}{2}\sum_{i=1}^m(y^i-\theta^Tx^i)^2$.  
(b) Show that the first iteration of Newton’s method gives us $\theta^*=(X^TX)^{-1}X^Ty^{->}$($y^{->}=[y^1,...,y^m]^T$) , the solution to our least squares problem.

(a).$\frac{\partial J^2(\theta)}{\partial \theta_j\theta_k}=\sum_{i=1}^mx_j^i*x_k^i=(X^TX)_{jk}$,其中$X=[x^1,...,x^m]$;    
(b).$\theta^1=\theta^0-H^{-1}\nabla J(\theta^0)=\theta^0-(X^TX)^{-1}*(X^TX\theta^0-X^Ty^{->})=(X^TX)^{-1}X^Ty^{->}$  

