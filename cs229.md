for cs229.  

lecture2 梯度下降  
主要公式:   
    $\theta_{i}:=\theta_{i}-\alpha*\frac{\partial}{\partial\theta}J(\theta)$  
    $J(\theta)$即为损失函数。另外，公式中的各个均为向量或矩阵或张量形式。 

术语：  
    a.LR(linear regression),线性回归。  
    b.SGD(stochostic gradient descent),随机梯度下降。每次选取一个batch的数据而不是全部的数据，之所以这样做是因为数据量太大，整体而言，SGD也是可以达到局部最优解附近的。    
    c.trace(A)，一个矩阵的迹。一个n*n矩阵的对角线上(左上至右下)元素的和即称为矩阵的迹。  
    d.normal equation,略。  

lecture3 过拟合、欠拟合  
主要公式：  
    1.lwr  
    $min\sum_iw^i*(y^{i}-\theta^Tx^{i})^2$  
    2.logistic regression sigmoid函数(或称为logistic函数)   
    $g(z)=\frac{1}{1+e^{-z}}$  
    $h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$  
    $P(y|x;\theta)=h_\theta(x)^y+(1-h_\theta(x))^{(1-y)}$  
    为了求得一组参数$\theta$,可以使用最大似然估计，即找到一组参数$\theta$使得出现数据集(即被观测的值)的概率最大：  
    $Max_{\theta}L(\theta)=\prod_iP(y^{i}|x^{i};\theta^{i})$  
    为了简化计算，可以取对数，即:  
    $Max_{\theta}logL(\theta)=\sum_{i}(y_i*logh_\theta(x^{i})+(1-y^i)*log(1-h_{\theta}(x^{i})))$  
    求最优解可以使用梯度上升法。  
术语：  
    a.LWR(local weighted regression),目标是拟合出一组参数$\theta$使得下式最小化：  
    $min\sum_iw^i*(y^{i}-\theta^Tx^{i})^2$,  
    其中，$w^i=e^{-\frac{(x-x^{i})^2}{2\tau^2}}$(当然也可以有其他选择)。    
    从上面的式子可以看出，若$x$离训练样本$x^i$越近，其所具有的权重$w^i$也就越大;而其中的$\tau$项用以控制权重函数的下降速率，$\tau$越小，下降的速率越快，权重函数的形状也就越窄。  
    每进行一次测试，即给定一个x，都需要重新对整个训练集运行算法一次以得到一组$\theta$值,而预测出来的值即为$\theta^Tx$,所以对于巨大的数据集而言，这个算法并不划算。     
    b.欠拟合，选择的特征数太少，不能很好的拟合数据。  
    c.过拟合，特征数太多，完美地拟合了训练数据，但是会在测试上表现的很差。  
    d.logistic regression。  

lecture4 牛顿方法  
公式：  
    1.newton法的$\theta$更新：(用以求得一个$\theta$使得$f(\theta)=0$)  
    $\theta^{(i+1)}:=\theta^{i}-\frac{f(\theta^{i})}{f^{'}(\theta^i)}$
    那么把它运用到logistic regression上，因为是要求最大似然函数$l(\theta)$，则对应一阶导数值$l^{'}(\theta)=0$,即可以令$f(\theta)=l^{'}(\theta)$,那么对应的$\theta$更新变为：  
    $\theta^{i+1}:=\theta^{i}-\frac{l^{'}(\theta)}{l^{''}(\theta)}$  
    另外，对于向量化的$\theta$，上式会变为：  
    $\theta^{(i+1)}:=\theta^{i}-H^{-1}\nabla_{\theta}l(\theta)$  
    其中,$H^{-1}$为一二阶偏导矩阵，称为hessian矩阵，矩阵中的每一项为：  
    $H_{ij}=\frac{\partial l^{2}(\theta)}{\partial \theta_i \partial\theta_j}$  
    注意到对于每一次迭代均需要计算和存储hessian矩阵，所以代价是比较高昂的(对于特征数量很多的情况而言)。  
    2.指数分布家族：  
    $p(y;\eta)=b(y)*e^{(\eta^T*T(y)-a(\eta))}$  
    其中,$\eta$被称为natural parameter;T(y)称为sufficient statistic，经常有:$T(y)=y$;$a(\eta)$称为log partition function;$e^{-a(\eta)}$作为一个归一化项使得$p(y;\eta)$分布在y上的和为1。
    接下来证明之前提到的bernoulli分布和Gaussian分布均属于指数分布家族：  
    设$\theta$为均值，则有：  
    a.Bernoulli分布：$P(y;\theta)=\theta^y*(1-\theta)^{(1-y)}$   
    经过简单推导可得：$P(y;\theta)=e^{(y*log\frac{\theta}{(1-\theta)}+log(1-\theta))}$    
    相应的对应关系一目了然；  
    b.Gaussian分布:$P(y;\theta)=\frac{1}{\sqrt(2\pi)}e^{\frac{-(y-\theta)^2}{2}}$(省略了$\delta^2$)  
    同理：$P(y;\theta)=\frac{1}{\sqrt(2\pi)}e^{-0.5y^2} *e^{(y\theta-0.5\theta^2)}$  
    对应关系也是一目了然。  
    实际上，多项式分布均属于指数分布家族。  
    3.从指数分布家族推导出一个广义线性模型  
    首先需要做三个假设：  
    a.$[y|x;\theta]\in exponential family(\eta)$;  
    b. 目标是给定x用以预测T(y)?对于大多数情况$T(y)=y$,即使得预测$h(x)=E[y|x]$;  
    c.$\eta=\theta^Tx$(对于向量化的版本：$\eta_i=\theta_i^Tx$)。    
    4.softmax regression 多项式分布建模   
    设:  
    $P(y=i;\phi)=\phi_i$;  
    $\sum_i\phi_i=1$;   
    首先定义$T(y)\in\Re^{k-1}$:  
    $T(1)=[1,0,0,...,0]^T$  
    $T(2)=[0,1,0,...,0]^T$  
    $T(k)=[0,0,0,...,0]^T$   
    其次定义indicator function(指示器函数):  
    $1\{True\}=1,1\{False\}=0$  
    那么有：  
    $T(y)_i=1\{y=i\}$  
    $T(y;\phi)=\phi^{1\{y=1\}}_{1}*...*\phi^{1\{y=k\}}_{k}$    
    $=exp(T(y)_1*log\phi_1+...+(1-\sum_{i=1}^{k-1}T(y)_i)*log\phi_k)$
    $=exp(T(y)_1*log(\phi_1/\phi_k)+...+T(y)_{k-1}*log(\phi_{k-1}/\phi_k)+log(\phi_k))$  
    那么，与相应的指数分布家族对应，有:  
    $b(y)=1$  
    $\eta=[log(\phi_1/\phi_k),...,log(\phi_{k-1}/\phi_k)]^T$  
    $a(\eta)=-log(\phi_k)$  
    容易倒推出：  
    $e^{\eta_i}=\frac{\phi_i}{\phi_k}$  
    $\phi_k\sum_{i=1}^{k}e^{\eta_i}=1$(总的概率和为1)  
    $\phi_i=\frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}}$  
    $P(y=i|x;\theta)=\phi_i=\frac{e^{\theta_i^Tx}}{\sum_{i=1}^{k}e^{\theta_j^Tx}}$  
    $h_\theta(x)=E[T(y)|x;\theta]=[\phi_1,...,\phi_{k-1}]T$  

术语：  
    a.newton method,不同于梯度下降法的另一种求解最值的方法，满足二次收敛性，但有一定适用条件。  
    b.generalized linear model,广义线性模型。最小二乘和logistic regression均为广义线性模型的一个特例。    
    c.指数分布家族(exponential family distributions)。  
    d.softmax regression。多分类问题。    
