for cs229.  

lecture2 梯度下降  
主要公式:   
    $\theta_{i}:=\theta_{i}-\alpha*\frac{\partial}{\partial\theta}J(\theta)$  
    $J(\theta)$即为损失函数。另外，公式中的各个均为向量或矩阵或张量形式。 

术语：  
    a.LR(linear regression),线性回归。  
    b.SGD(stochostic gradient descent),随机梯度下降。每次选取一个batch的数据而不是全部的数据，之所以这样做是因为数据量太大，整体而言，SGD也是可以达到局部最优解附近的。    
    c.trace(A)，一个矩阵的迹。一个n*n矩阵的对角线上(左上至右下)元素的和即称为矩阵的迹。  
    d.normal equation,略。  

lecture3 过拟合、欠拟合  
主要公式：  
    1.lwr  
    $min\sum_iw^i*(y^{i}-\theta^Tx^{i})^2$  
    2.logistic regression sigmoid函数(或称为logistic函数)   
    $g(z)=\frac{1}{1+e^{-z}}$  
    $h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$  
    $P(y|x;\theta)=h_\theta(x)^y+(1-h_\theta(x))^{(1-y)}$  
    为了求得一组参数$\theta$,可以使用最大似然估计，即找到一组参数$\theta$使得出现数据集(即被观测的值)的概率最大：  
    $Max_{\theta}L(\theta)=\prod_iP(y^{i}|x^{i};\theta^{i})$  
    为了简化计算，可以取对数，即:  
    $Max_{\theta}logL(\theta)=\sum_{i}(y_i*logh_\theta(x^{i})+(1-y^i)*log(1-h_{\theta}(x^{i})))$  

术语：  
    a.LWR(local weighted regression),目标是拟合出一组参数$\theta$使得下式最小化：  
    $min\sum_iw^i*(y^{i}-\theta^Tx^{i})^2$,  
    其中，$w^i=e^{-\frac{(x-x^{i})^2}{2\tau^2}}$(当然也可以有其他选择)。    
    从上面的式子可以看出，若$x$离训练样本$x^i$越近，其所具有的权重$w^i$也就越大;而其中的$\tau$项用以控制权重函数的下降速率，$\tau$越小，下降的速率越快，权重函数的形状也就越窄。  
    每进行一次测试，即给定一个x，都需要重新对整个训练集运行算法一次以得到一组$\theta$值,而预测出来的值即为$\theta^Tx$,所以对于巨大的数据集而言，这个算法并不划算。     
    b.欠拟合，选择的特征数太少，不能很好的拟合数据。
    c.过拟合，特征数太多，完美地拟合了训练数据，但是会在测试上表现的很差。  
    d.logistic regression。