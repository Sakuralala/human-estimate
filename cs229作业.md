*作业1*  
1.In this problem, we will prove that if we use Newton’s method solve the least squares
optimization problem, then we only need one iteration to converge to $\theta^*$.  
(a) Find the Hessian of the cost function $J(θ)=\frac{1}{2}\sum_{i=1}^m(y^i-\theta^Tx^i)^2$.  
(b) Show that the first iteration of Newton’s method gives us $\theta^*=(X^TX)^{-1}X^Ty^{->}$($y^{->}=[y^1,...,y^m]^T$) , the solution to our least squares problem.

(a).$\frac{\partial J^2(\theta)}{\partial \theta_j\theta_k}=\sum_{i=1}^mx_j^i*x_k^i=(X^TX)_{jk}$,其中$X=[x^1,...,x^m]$;    
(b).$\theta^1=\theta^0-H^{-1}\nabla J(\theta^0)=\theta^0-(X^TX)^{-1}*(X^TX\theta^0-X^Ty^{->})=(X^TX)^{-1}X^Ty^{->}$  

2. Locally-weighted logistic regression  
$l(\theta)=\frac{-\lambda}{2}\theta^T\theta+\sum_{i=1}^mw^i[y^ilogh_\theta(x^i)+(1-y^i)log(1-h_\theta(x^i)])$  
求$\nabla_{\theta}l(\theta)$.  
注意此处：  
a.链式法则的运用；  
b.向量化的表示。(先写出非向量化的表示，然后根据矩阵相乘的规律确定向量化的表示)  

后面的感觉数学计算证明比较多，暂时先跳了。。。  